{"cells":[{"cell_type":"markdown","source":["# Mount Google Drive"],"metadata":{"id":"k9gycXlunD2k"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mz-RW3ECnBrg","executionInfo":{"status":"ok","timestamp":1758111555829,"user_tz":-120,"elapsed":52885,"user":{"displayName":"Sian C","userId":"07859676698101222721"}},"outputId":"3a5b9576-60fc-4a5b-98f1-36d53e89c399"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"AjLkwPN60UYv"},"source":["# Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AhwN4yFz0WGX"},"outputs":[],"source":["# ---------------\n","# --- Imports ---\n","# ---------------\n","\n","import pandas as pd\n","import numpy as np\n","import os\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils import shuffle\n","import pandas as pd\n","from sklearn.preprocessing import LabelEncoder\n","from imblearn.over_sampling import ADASYN\n","from sklearn.utils import resample\n","from collections import Counter\n","from imblearn.utils import check_sampling_strategy\n"]},{"cell_type":"markdown","source":["# File Paths"],"metadata":{"id":"EysITIyrtUeX"}},{"cell_type":"code","source":["# Get Data file path\n","file_path = 'cicids2017_cleaned.csv' # Set to full file directory location of orginal preprocessed CICIDS2017 dataset by ERIC ANACLETO RIBEIRO\n","\n","baseline_save_path = \"/content/drive/MyDrive/CNXSIA001_LAIDS_SOURCE_CODE/Baseline 1D CNN Model Files/Datasets\" # A pca-cnn dataset save path is not needed as pca is applied to the baseline's dataset and then saved as seperate pca-cnn datasets\n","ae_mlp_save_path = \"/content/drive/MyDrive/CNXSIA001_LAIDS_SOURCE_CODE/AE-MLP Model Files/Datasets and Numpy Arrays\"\n","os.makedirs(baseline_save_path, exist_ok=True)\n","os.makedirs(ae_mlp_save_path, exist_ok=True)"],"metadata":{"id":"QMRvbQyMtW8q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e1henyAM0QOc"},"source":["# Load in Datset\n","\n","Dataset developed by ERIC ANACLETO RIBEIRO can be downloaded from: https://www.kaggle.com/datasets/ericanacletoribeiro/cicids2017-cleaned-and-preprocessed"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ti5utv6qwyfa","outputId":"04596505-5d3c-44da-bc61-3f99434e9625"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Attack Type Distribution:\n","Attack Type\n","Normal Traffic    2095057\n","DoS                193745\n","DDoS               128014\n","Port Scanning       90694\n","Brute Force          9150\n","Web Attacks          2143\n","Bots                 1948\n","Name: count, dtype: int64\n","\n","Total Samples: 2520751\n","Total Benign Samples: 2095057\n","Total Malicious Samples: 425694\n","\n","Label Encoding Mapping:\n","{'Normal Traffic': 0, 'Port Scanning': 1, 'Web Attacks': 2, 'Brute Force': 3, 'DDoS': 4, 'Bots': 5, 'DoS': 6}\n"]}],"source":["# --------------------------\n","# --- Load in CICIDS2017 ---\n","# --------------------------\n","\n","# Read in the preprocessed CICIDS2017 dataset as a panda dataframe\n","cicids2017_df = pd.read_csv(file_path, sep=\",\", comment=\"#\", header=0)\n","cicids2017_df.columns = cicids2017_df.columns.str.strip()  # Strip any whitespaces from column names\n","\n","print(\"\\nAttack Type Distribution:\")\n","attack_type_counts = cicids2017_df['Attack Type'].value_counts()\n","print(attack_type_counts)\n","\n","# Calc. and print total benign and malicious samples\n","total_benign = attack_type_counts.get('Normal Traffic', 0)\n","total_malicious = attack_type_counts.sum() - total_benign\n","\n","print(f\"\\nTotal Samples: {total_benign + total_malicious}\")\n","print(f\"Total Benign Samples: {total_benign}\")\n","print(f\"Total Malicious Samples: {total_malicious}\")\n","\n","# ----------------------\n","# --- Label Encoding ---\n","# ----------------------\n","\n","attack_type_map = {'Normal Traffic': 0, 'Port Scanning': 1, 'Web Attacks': 2, 'Brute Force': 3, 'DDoS': 4, 'Bots': 5, 'DoS': 6} # Create a mapping from attack type to an integer label\n","\n","cicids2017_df['Attack Type'] = cicids2017_df['Attack Type'].map(attack_type_map)# Apply the label encoding\n","\n","# Display the encodings\n","print(\"\\nLabel Encoding Mapping:\")\n","print(attack_type_map)\n","\n","# -----------------------------------------\n","# --- Split the Labels from the Samples ---\n","# -----------------------------------------\n","X = cicids2017_df.drop('Attack Type', axis=1)\n","y = cicids2017_df['Attack Type']\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"almN9-j-Rw1f"},"outputs":[],"source":["# ---------------------------------------------\n","# --- Separate Benign and Malicious Traffic ---\n","# ---------------------------------------------\n","\n","# Separate benign and malicious samples into seperate dataframes\n","benign_df = cicids2017_df[cicids2017_df['Attack Type'] == 0]\n","malicious_df = cicids2017_df[cicids2017_df['Attack Type'] != 0]\n","\n","# Separate features and labels for benign data\n","X_benign = benign_df.drop('Attack Type', axis=1) # Samples\n","y_benign = benign_df['Attack Type'] # Labels\n","\n","# Separate features and labels for malicious data\n","X_malicious = malicious_df.drop('Attack Type', axis=1) # Samples\n","y_malicious = malicious_df['Attack Type'] # Labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Pb34miBBKUr"},"outputs":[],"source":["# ---------------------------------\n","# --- Train_Val_Test_splits set ---\n","# ---------------------------------\n","X_benign_temp, X_test_benign, y_benign_temp, y_test_benign = train_test_split(X_benign, y_benign, test_size=0.10, random_state=42) # 10% of the benign data allocated to the test set\n","\n","X_benign_temp, X_classifier_train_benign, y_benign_temp, y_classifier_train_benign = train_test_split(X_benign_temp, y_benign_temp, test_size=0.20, random_state=42) # 80:20 split of rest of the benign data between the AE and classifier sets (because the benign dataset is so large)\n","\n","X_classifier_train_benign, X_classifier_val_benign, y_classifier_train_benign, y_classifier_val_benign = train_test_split(X_classifier_train_benign, y_classifier_train_benign, test_size=0.20, random_state=42) # 80:20 split of the classifier's benign data allocation between its training and validation set\n","\n","X_train, X_val, y_train, y_val = train_test_split(X_benign_temp, y_benign_temp, test_size=0.10, random_state=42) #90:10 split of the AE's benign data allocaton between it's training and validation class (because the benign dataset is so large)\n","\n","X_mal_temp, X_malicious_test, y_mal_temp, y_malicious_test = train_test_split(X_malicious, y_malicious, test_size=0.2, random_state=42, stratify = y_malicious) # 20% of the malicious data samples are allocated to the test set\n","\n","X_mal_train, X_mal_val, y_mal_train, y_mal_val = train_test_split(X_mal_temp, y_mal_temp, test_size=0.20, random_state=42, stratify = y_mal_temp) # 80:20 split of the classifiers' malicious data allocation between its training and validation set\n","\n","\n","# Concatenate benign and malicious dataframes together for the classifier and test sets\n","X_test = pd.concat([X_test_benign, X_malicious_test], ignore_index=True)\n","y_test = pd.concat([y_test_benign, y_malicious_test], ignore_index=True)\n","\n","X_classifier_train = pd.concat([X_classifier_train_benign, X_mal_train], ignore_index=True)\n","y_classifier_train = pd.concat([y_classifier_train_benign, y_mal_train], ignore_index=True)\n","\n","X_classifier_val = pd.concat([X_classifier_val_benign, X_mal_val], ignore_index=True)\n","y_classifier_val = pd.concat([y_classifier_val_benign, y_mal_val], ignore_index=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wD2-lUwbx9rL"},"outputs":[],"source":["# ------------------------\n","# --- Shuffle the data ---\n","# ------------------------\n","\n","AE_X_train, AE_y_train = shuffle(X_train, y_train, random_state=42) #Autoencoder\n","AE_X_val, AE_y_val = shuffle(X_val, y_val, random_state=42) #Autoencoder\n","\n","Classifier_X_train, Classifier_y_train = shuffle(X_classifier_train, y_classifier_train, random_state=42) #Classifier\n","Classifier_X_val, Classifier_y_val = shuffle(X_classifier_val, y_classifier_val, random_state=42) #Classifier\n","\n","X_test, y_test = shuffle(X_test, y_test, random_state=42) #Test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_jPHO9ZQx_gG"},"outputs":[],"source":["# -----------------------\n","# ---- Reset Indexes ----\n","# -----------------------\n","\n","AE_X_train = AE_X_train.reset_index(drop=True)\n","AE_y_train = AE_y_train.reset_index(drop=True)\n","\n","AE_X_val = AE_X_val.reset_index(drop=True)\n","AE_y_val = AE_y_val.reset_index(drop=True)\n","\n","Classifier_X_train = Classifier_X_train.reset_index(drop=True)\n","Classifier_y_train = Classifier_y_train.reset_index(drop=True)\n","\n","Classifier_X_val = Classifier_X_val.reset_index(drop=True)\n","Classifier_y_val = Classifier_y_val.reset_index(drop=True)\n","\n","X_test = X_test.reset_index(drop=True)\n","y_test = y_test.reset_index(drop=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8A2UqVCdzYLb","outputId":"524d29ee-c556-47d6-dcac-318b149b6b8e"},"outputs":[{"name":"stdout","output_type":"stream","text":["AE Training set class distribution:\n","Attack Type\n","Normal Traffic    1357596\n","Name: count, dtype: int64 \n","\n","AE Validation set class distribution:\n","Attack Type\n","Normal Traffic    150844\n","Name: count, dtype: int64 \n","\n","Classifier Training set class distribution:\n","Attack Type\n","Normal Traffic    301688\n","DoS               123997\n","DDoS               81929\n","Port Scanning      58044\n","Brute Force         5856\n","Web Attacks         1371\n","Bots                1247\n","Name: count, dtype: int64 \n","\n","Classifier Validation set class distribution:\n","Attack Type\n","Normal Traffic    75423\n","DoS               30999\n","DDoS              20482\n","Port Scanning     14511\n","Brute Force        1464\n","Web Attacks         343\n","Bots                312\n","Name: count, dtype: int64 \n","\n","Test set class distribution:\n","Attack Type\n","Normal Traffic    209506\n","DoS                38749\n","DDoS               25603\n","Port Scanning      18139\n","Brute Force         1830\n","Web Attacks          429\n","Bots                 389\n","Name: count, dtype: int64 \n","\n"]}],"source":["# --------------------------------\n","# ---- Show Data Distribution ----\n","# --------------------------------\n","\n","reverse_attack_type_map = {v: k for k, v in attack_type_map.items()} # Invert the dictionary to map int -> attack name\n","\n","# Map encoded labels values to the orginal label names\n","AE_y_train_named = AE_y_train.map(reverse_attack_type_map)\n","AE_y_val_named   = AE_y_val.map(reverse_attack_type_map)\n","\n","Classifier_y_train_named = Classifier_y_train.map(reverse_attack_type_map)\n","Classifier_y_val_named   = Classifier_y_val.map(reverse_attack_type_map)\n","\n","y_test_named = y_test.map(reverse_attack_type_map)\n","\n","# Show distributions\n","print(\"AE Training set class distribution:\")\n","print(AE_y_train_named.value_counts(), \"\\n\")\n","\n","print(\"AE Validation set class distribution:\")\n","print(AE_y_val_named.value_counts(), \"\\n\")\n","\n","print(\"Classifier Training set class distribution:\")\n","print(Classifier_y_train_named.value_counts(), \"\\n\")\n","\n","print(\"Classifier Validation set class distribution:\")\n","print(Classifier_y_val_named.value_counts(), \"\\n\")\n","\n","print(\"Test set class distribution:\")\n","print(y_test_named.value_counts(), \"\\n\")\n"]},{"cell_type":"markdown","metadata":{"id":"WbvSgpG40z8R"},"source":["# Apply ADASYN to the Classfier Training Set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GuEsUhTRXYty","outputId":"ec5ec92e-1f93-4a94-b956-5aec4c8d0354"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Class distribution after ADASYN (numeric):\n","Counter({6: 301889, 4: 301715, 5: 301702, 2: 301702, 3: 301697, 0: 301688, 1: 301212})\n","\n","Final dataset shape: (2111605, 52)\n","\n","Final class distribution:\n","Counter({6: 301889, 4: 301715, 5: 301702, 2: 301702, 3: 301697, 0: 301688, 1: 301212})\n"]}],"source":["# Create temp vars\n","X_down = Classifier_X_train\n","y_down = Classifier_y_train\n","\n","# Get class counts\n","class_counts = y_down.value_counts().to_dict()\n","majority_class_count = class_counts.get(0, max(class_counts.values()))  # 0 = benign\n","\n","# ---------------------------------\n","# --- Apply ADASYN oversampling ---\n","# ---------------------------------\n","adasyn = ADASYN(sampling_strategy='auto', # Specify which classes to oversample\n","                n_neighbors=5,            # Number of neighbors used in ADASYN\n","                random_state=42)\n","\n","X_res, y_res = adasyn.fit_resample(X_down, y_down) # Perform oversampling\n","\n","print(\"\\nClass distribution after ADASYN (numeric):\")\n","print(Counter(y_res))\n","\n","\n","# Convert results back to pandas DataFrame\n","X_resampled = pd.DataFrame(X_res, columns=X_down.columns)\n","y_resampled = pd.DataFrame(y_res, columns=['Attack Type'])\n","\n","print(f\"\\nFinal dataset shape: {X_resampled.shape}\")\n","print(\"\\nFinal class distribution:\")\n","print(Counter(y_resampled['Attack Type']))\n","\n","Adasyn_Classifier_X_train = X_resampled\n","Adasyn_Classifier_y_train = y_resampled\n"]},{"cell_type":"markdown","metadata":{"id":"sY5YuJVqaBrc"},"source":["#Normalise and Save Baseline and PCA-CNN Datasets\n","\n","(normalise using Standard Scaler)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2CZySGHUbtl-","outputId":"02d060a2-1a40-4bfb-d763-f71a4f41edd1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Adasyn datasets shuffled and re-indexed!\n"]}],"source":["# --- Shuffle & reset index Training Set --- (just incase)\n","Adasyn_Classifier_X_train, Adasyn_Classifier_y_train = shuffle(Adasyn_Classifier_X_train, Adasyn_Classifier_y_train, random_state=42)\n","Adasyn_Classifier_X_train = Adasyn_Classifier_X_train.reset_index(drop=True)\n","Adasyn_Classifier_y_train = Adasyn_Classifier_y_train.reset_index(drop=True)\n","\n","print(\"Adasyn datasets shuffled and re-indexed!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WvnCEdwFkCrP","outputId":"ff4be23d-4982-4bf5-d094-1c753746c853"},"outputs":[{"name":"stdout","output_type":"stream","text":["Baseline datasets saved successfully!\n"]}],"source":["from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","\n","# --------------------------------------\n","# --- StandardScaler Standardisation ---\n","# --------------------------------------\n","\n","scaler_standard = StandardScaler()\n","\n","# Apply standardisation\n","adasyn_baseline_X_train = scaler_standard.fit_transform(Adasyn_Classifier_X_train)\n","baseline_X_val = scaler_standard.transform(Classifier_X_val)\n","baseline_X_test = scaler_standard.transform(X_test)\n","\n","# Assign labels to corresponding vars\n","adasyn_baseline_y_train = Adasyn_Classifier_y_train\n","baseline_y_val = Classifier_y_val\n","baseline_y_test = y_test\n","\n","\n","# ----------------------------------\n","# --- Save datasets ---\n","# ----------------------------------\n","\n","# ADASYN-resampled baseline CNN training set\n","pd.DataFrame(adasyn_baseline_X_train).to_csv(os.path.join(baseline_save_path, \"adasyn_baseline_X_train.csv\"), index=False)\n","pd.DataFrame(adasyn_baseline_y_train).to_csv(os.path.join(baseline_save_path, \"adasyn_baseline_y_train.csv\"), index=False)\n","\n","# Validation set\n","pd.DataFrame(baseline_X_val).to_csv(os.path.join(baseline_save_path, \"baseline_X_val.csv\"), index=False)\n","pd.DataFrame(baseline_y_val).to_csv(os.path.join(baseline_save_path, \"baseline_y_val.csv\"), index=False)\n","\n","# Test set\n","pd.DataFrame(baseline_X_test).to_csv(os.path.join(baseline_save_path, \"baseline_X_test.csv\"), index=False)\n","pd.DataFrame(baseline_y_test).to_csv(os.path.join(baseline_save_path, \"baseline_y_test.csv\"), index=False)\n","\n","print(\"Baseline datasets saved successfully!\")"]},{"cell_type":"markdown","metadata":{"id":"KzUVvF2faHZj"},"source":["#Normalise and save AE-MLP Datasets\n","\n","(normalise using MinMaxScaler)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"guZlsNNnZyz2","outputId":"9301c52a-8c2a-47fe-eb85-d0c2d263849f"},"outputs":[{"name":"stdout","output_type":"stream","text":["All AE-MLP and ADASYN MLP datasets saved successfully!\n"]}],"source":["from sklearn.preprocessing import MinMaxScaler\n","import os\n","import pandas as pd\n","\n","# ----------------------------------\n","# --- MinMaxScaler Normalisation ---\n","# ----------------------------------\n","scaler_minmax = MinMaxScaler()\n","\n","# Apply normalisation\n","\n","# AE sets\n","ae_X_train = scaler_minmax.fit_transform(AE_X_train)\n","ae_X_val = scaler_minmax.transform(AE_X_val)\n","\n","# Classifier sets\n","adasyn_mlp_X_train = scaler_minmax.transform(Adasyn_Classifier_X_train)\n","mlp_X_val = scaler_minmax.transform(Classifier_X_val)\n","\n","# Test set\n","ae_mlp_X_test = scaler_minmax.transform(X_test)\n","\n","# Assign labels to vars\n","ae_y_train = AE_y_train\n","ae_y_val   = AE_y_val\n","adasyn_mlp_y_train = Adasyn_Classifier_y_train\n","mlp_y_val  = Classifier_y_val\n","ae_mlp_y_test = y_test\n","\n","# ---------------------\n","# --- Save datasets ---\n","# ---------------------\n","\n","# AE sets\n","pd.DataFrame(ae_X_train).to_csv(os.path.join(ae_mlp_save_path, \"ae_x_train.csv\"), index=False)\n","pd.DataFrame(ae_y_train).to_csv(os.path.join(ae_mlp_save_path, \"ae_y_train.csv\"), index=False)\n","pd.DataFrame(ae_X_val).to_csv(os.path.join(ae_mlp_save_path, \"ae_x_val.csv\"), index=False)\n","pd.DataFrame(ae_y_val).to_csv(os.path.join(ae_mlp_save_path, \"ae_y_val.csv\"), index=False)\n","\n","# ADASYN-resampled Classifier training sets\n","pd.DataFrame(adasyn_mlp_X_train).to_csv(os.path.join(ae_mlp_save_path, \"adasyn_mlp_x_train.csv\"), index=False)\n","pd.DataFrame(adasyn_mlp_y_train).to_csv(os.path.join(ae_mlp_save_path, \"adasyn_mlp_y_train.csv\"), index=False)\n","\n","# Classifier validation sets\n","pd.DataFrame(mlp_X_val).to_csv(os.path.join(ae_mlp_save_path, \"mlp_x_val.csv\"), index=False)\n","pd.DataFrame(mlp_y_val).to_csv(os.path.join(ae_mlp_save_path, \"mlp_y_val.csv\"), index=False)\n","\n","# Test sets\n","pd.DataFrame(ae_mlp_X_test).to_csv(os.path.join(ae_mlp_save_path, \"ae_mlp_x_test.csv\"), index=False)\n","pd.DataFrame(ae_mlp_y_test).to_csv(os.path.join(ae_mlp_save_path, \"ae_mlp_y_test.csv\"), index=False)\n","\n","print(\"All AE-MLP and ADASYN MLP datasets saved successfully!\")"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.13.3"}},"nbformat":4,"nbformat_minor":0}