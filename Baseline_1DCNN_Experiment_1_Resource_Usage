{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Run Instructions:\n","\n","\n","\n","*   Connect to google drive to access files.\n","*   Change file paths to where the datasets (dataset_path) and model files (save_path) are stored.\n","*   Run one model at a time (comment other model code out) to get accurate measurements.\n","*   Restart runtime environement ('Restart session and run all') before every measurement is taken.\n","\n"],"metadata":{"id":"kFs2j0ZzVDRI"}},{"cell_type":"markdown","source":["# Mount Google Drive"],"metadata":{"id":"OM0dAvCRMMzM"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m8WPMUHwSryH","executionInfo":{"status":"ok","timestamp":1758120535631,"user_tz":-120,"elapsed":22517,"user":{"displayName":"Sian C","userId":"07859676698101222721"}},"outputId":"fdf71d0a-3189-4a91-e125-5c1f841ebe82","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["#File Paths"],"metadata":{"id":"WrUkHWOPA0Ij"}},{"cell_type":"code","source":["save_path = \"/content/drive/MyDrive//CNXSIA001_LAIDS_SOURCE_CODE/Baseline 1D CNN Model Files/Models\" # Model path\n","dataset_path =  \"/content/drive/MyDrive/CNXSIA001_LAIDS_SOURCE_CODE/Baseline 1D CNN Model Files/Datasets\""],"metadata":{"id":"Eqpx_eRoA-6X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import os\n","import numpy as np\n","\n","# ---------------------\n","# --- Load Datasets ---\n","# ---------------------\n","\n","# Test sets\n","X_test = pd.read_csv(os.path.join(dataset_path, \"baseline_X_test.csv\")).to_numpy()\n","y_test = pd.read_csv(os.path.join(dataset_path, \"baseline_y_test.csv\")).to_numpy().ravel()\n","\n","# Get index of first malicious sample\n","first_malicious_index = np.where(y_test != 0)[0][0]\n","\n","# Extract the one sample\n","X_test_sample = X_test[first_malicious_index]\n","y_test_sample = y_test[first_malicious_index]\n","\n","print(f\"X_test_sample: {X_test_sample}\")\n","print(f\"y_test_sample: {y_test_sample}\")\n","\n","# Free up memory by deleting the datasets\n","del X_test, y_test\n","import gc; gc.collect()"],"metadata":{"id":"3b1doEX5Zfuy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Set CPU Constraints"],"metadata":{"id":"Pqmc4TasVi9A"}},{"cell_type":"code","source":["import os\n","# limit libraries' CPU thread requests to up to 4 threads max to use for parallel computations\n","\n","os.environ['TF_NUM_INTRAOP_THREADS'] = '4' # Only allow TensorFlow to request a max 4 CPU threads per operation (if only 2 threads exist, it only 2 threads are used)\n","os.environ['TF_NUM_INTEROP_THREADS'] = '1'  # Forces TensorFlow to run only one operation at a time\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # reduce Tensorflows output to the console\n","\n","os.environ['OMP_NUM_THREADS'] = '4' # Set the max number of threads the OpenMP (which uses numerical libraries such as NumPy) can request to 4\n","os.environ['MKL_NUM_THREADS'] = '4' # Set the maximum number of threads Intel MKL (Math Kernel Library) can request to 4\n","os.environ['OPENBLAS_NUM_THREADS'] = '4' # Set the maximum number of threads OpenBLAS can request to 4.\n","\n"],"metadata":{"id":"hp0k39XOl1TI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Imports"],"metadata":{"id":"D2NihRdLm-oV"}},{"cell_type":"code","source":["# ---------------\n","# --- Imports ---\n","# ---------------\n","\n","import os\n","import psutil\n","import time\n","import resource\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","from tensorflow.keras.models import load_model, Sequential\n","from tensorflow.keras import layers, regularizers\n","from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n","from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n"],"metadata":{"id":"Hprzx6shm_js"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Set resource constraints"],"metadata":{"id":"7RvkTflk3B2S"}},{"cell_type":"code","source":["# -------------------------------------------\n","# --- Google Colab Resource Configuration ---\n","# -------------------------------------------\n","\n","def configure_resources_for_colab():\n","    print(\"Configuring Google Colab resources...\")\n","\n","    # Restrict the number of CPU cores to use at most 4\n","    try:\n","        available_cores = min(4, os.cpu_count()) # Limit the CPU cores to be at most 4\n","        os.sched_setaffinity(0, list(range(available_cores))) #limit this current process (0) to run only on the list of available_cores CPU cores.\n","        print(\"Logical cores:\", psutil.cpu_count(logical=True))\n","        print(\"Physical cores:\", psutil.cpu_count(logical=False))\n","    except Exception as e:\n","        print(f\"CPU affinity warning: {e}\")\n","\n","    # Limit Memory to 4GB\n","    try:\n","        memory_limit = 4 * (1024**3)  # 4GB in bytes\n","        resource.setrlimit(resource.RLIMIT_AS, (memory_limit, memory_limit)) #sets the maximum allowed virtual memory this process can use to 4GB (set both soft and hard limits to 4 GB)\n","        print(f\"Memory limit set to 4GB\")\n","    except Exception as e:\n","        print(f\"Memory limit warning: {e}\")\n","\n","    # Print CPU informational\n","    try:\n","        cpu_info = psutil.cpu_freq()\n","        if cpu_info:\n","            print(f\"CPU frequency: {cpu_info.current:.0f}MHz\")\n","        else:\n","            print(f\"CPU frequency info not available\")\n","    except:\n","        print(f\"CPU frequency info not available\")\n","\n","    # Adjust CPU process priority\n","    try:\n","        os.nice(5)  # Lower the CPU priority of the process\n","        print(f\"Process priority lowered\")\n","    except Exception as e:\n","        print(f\"Priority adjustment warning: {e}\")\n","\n","    print(\"Resource configuration complete!\")\n","\n","# Execute function\n","configure_resources_for_colab()\n","\n","\n","# --------------------------------\n","# --- TensorFlow configuration ---\n","# --------------------------------\n","\n","def configure_tensorflow_colab():\n","    print(\"Configuring TensorFlow resources usage...\")\n","\n","    # Reenforce that tensorflow CPU thread usage is limited\n","    try:\n","        tf.config.threading.set_intra_op_parallelism_threads(4) # Only allow TensorFlow to request a max 4 CPU threads per operation (if only 2 threads exist, it only 2 threads are used)\n","        tf.config.threading.set_inter_op_parallelism_threads(1) # Forces TensorFlow to run only one operation at a time\n","        print(\"TensorFlow CPU thread limits configured\")\n","    except RuntimeError as e:\n","        print(\"CPU thread request error\")\n","\n","    tf.get_logger().setLevel('WARNING')# Limit the amount of info tf prints to the console\n","\n","    print(\"TensorFlow configuration complete!\")\n","\n","# Execute function\n","configure_tensorflow_colab()\n","\n","\n","# Get current Colab resource usages\n","def monitor_colab_resources():\n","    try:\n","        # Get memory usage\n","        mem = psutil.virtual_memory()\n","        print(f\"Memory: {mem.used / (1024**3):.1f}GB used out of {mem.total / (1024**3):.1f}GB total ({mem.percent:.1f}%)\")\n","\n","        # Get CPU usage\n","        cpu_percent = psutil.cpu_percent(interval=1)\n","        print(f\"CPU usage: {cpu_percent:.1f}%\")\n","\n","        # Get Disk space\n","        disk = psutil.disk_usage('/')\n","        print(f\"Disk: {disk.used / (1024**3):.1f}GB used out of {disk.total/(1024**3):.1f}GB total ({disk.percent:.1f}%)\")\n","\n","    except Exception as e:\n","        print(f\"Error: {e}\")\n","\n","# Execute function\n","monitor_colab_resources()"],"metadata":{"id":"uirBBsWgZB3U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Resource usage evaluation"],"metadata":{"id":"p8KozpUc213I"}},{"cell_type":"markdown","source":["#FP32 Baseline CNN\n","\n","\n","\n"],"metadata":{"id":"RC-NBwn1H2w4"}},{"cell_type":"code","source":["import os\n","import time\n","import numpy as np\n","import psutil\n","import tensorflow as tf\n","import gc\n","import statistics\n","\n","# -------------------\n","# --- Load models ---\n","# -------------------\n","\n","baseline_model_base = os.path.join(save_path, \"Best_Baseline\")\n","\n","tflite_models = [\n","    {\n","        \"model_path\": baseline_model_base + \"_float32.tflite\",\n","        \"model_name\": \"Baseline Float32 Model\",\n","    }\n","]\n","\n","process = psutil.Process(os.getpid()) # Initialise process instance\n","\n","# ----------------------------\n","# --- Run multiple trials ---\n","# ----------------------------\n","runs = 1000\n","for m in tflite_models:\n","    start_mem = process.memory_info().rss / (1024**2) # Get initialse memeory usage values\n","\n","    # --- Load interpreters ---\n","    interpreter = tf.lite.Interpreter(model_path=m[\"model_path\"])\n","    interpreter.allocate_tensors()\n","    input_details = interpreter.get_input_details()\n","    output_details = interpreter.get_output_details()\n","\n","    wall_times, the_cpu_seconds = [], []\n","\n","    for _ in range(runs):\n","        # Record resource usage\n","        start_time = time.time()\n","        start_cpu_times = process.cpu_times()\n","\n","\n","        # --- Inference ---\n","\n","        # Shape input\n","        sample_input = np.expand_dims(X_test_sample, axis=0)   # (1, features)\n","        sample_input = np.expand_dims(sample_input, axis=-1)   # (1, features, 1)\n","        sample_input = sample_input.astype(input_details[0]['dtype'])\n","\n","        interpreter.set_tensor(input_details[0]['index'], sample_input)\n","        interpreter.invoke()\n","        output_data = interpreter.get_tensor(output_details[0]['index'])\n","\n","        y_pred = np.argmax(output_data, axis=1)\n","\n","        # Record resource usage\n","        end_time = time.time()\n","        end_cpu_times = process.cpu_times()\n","\n","\n","        # Compute resource usage totals\n","        wall_clock = end_time - start_time\n","        cpu_seconds = (end_cpu_times.user + end_cpu_times.system) - (start_cpu_times.user + start_cpu_times.system)\n","\n","        # Appened run of the sample's times to a list\n","        wall_times.append(wall_clock)\n","        the_cpu_seconds.append(cpu_seconds)\n","\n","    # Get memory usage\n","    end_mem = process.memory_info().rss / (1024**2)\n","    total_ram_change = end_mem - start_mem # get total memory used to process samples 1000 times\n","    ram_change_per_sample = total_ram_change / runs # Get per sample RAM estimation\n","\n","    # --- Report average results ---\n","    print(f\"\\nResults for {m['model_name']} over {runs} runs:\")\n","    print(f\"Inference wall-clock time: {statistics.mean(wall_times):.10f} ± {statistics.stdev(wall_times):.10f} sec\")\n","    print(f\"CPU seconds: {statistics.mean(the_cpu_seconds):.10f} ± {statistics.stdev(the_cpu_seconds):.10f} sec\")\n","\n","    print(f\"RAM change: {total_ram_change:.10f} MB\")\n","    print(f\"RAM change per sample: {ram_change_per_sample:.10f} MB\")\n","\n","    print(f\"\\nResults for {m['model_name']} over {runs} runs:\")\n","    print(f\"Inference wall-clock time: {statistics.mean(wall_times)*1000:.4f} ± {statistics.stdev(wall_times)*1000:.4f} ms\")\n","    print(f\"CPU time: {statistics.mean(the_cpu_seconds)*1000:.4f} ± {statistics.stdev(the_cpu_seconds)*1000:.4f} ms\")\n","\n","    print(f\"RAM change over {runs} runs: {total_ram_change*1024:.4f} KB\")\n","    print(f\"RAM change per sample: {ram_change_per_sample*1024:.4f} KB\")\n","\n","    # Model file sizes\n","    baseline_size_mb = os.path.getsize(m[\"model_path\"]) / (1024**2)\n","    print(f\"Baseline FP32 model size: {baseline_size_mb:.4f} MB\")\n","\n"],"metadata":{"id":"xxRadtEMFl-p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["FP32 Results\n","\n","\n","\n","```\n","Results for Baseline Float32 Model over 1000 runs:\n","Inference wall-clock time: 0.0655 ± 0.0477 ms\n","CPU time: 0.0600 ± 0.7727 ms\n","CPU usage: 48.2645% ± 630.4753%\n","RAM change over 1000 runs: 3264.0000 KB\n","RAM change per sample: 3.2640 KB\n","PCA-CNN model size: 0.0194 MB\n","```\n","\n","\n","```\n","Results for Baseline Float32 Model over 1000 runs:\n","Inference wall-clock time: 0.0598 ± 0.0239 ms\n","CPU time: 0.0700 ± 0.8341 ms\n","CPU usage: 59.1653% ± 708.5737%\n","RAM change over 1000 runs: 3840.0000 KB\n","RAM change per sample: 3.8400 KB\n","PCA-CNN model size: 0.0194 MB\n","```\n","\n","\n","\n","```\n","Results for Baseline Float32 Model over 1000 runs:\n","Inference wall-clock time: 0.0616 ± 0.0209 ms\n","CPU time: 0.0700 ± 0.8341 ms\n","CPU usage: 50.0198% ± 607.0097%\n","RAM change over 1000 runs: 3672.0000 KB\n","RAM change per sample: 3.6720 KB\n","PCA-CNN model size: 0.0194 MB\n","```\n","\n","\n","\n","```\n","Results for Baseline Float32 Model over 1000 runs:\n","Inference wall-clock time: 0.0624 ± 0.0219 ms\n","CPU time: 0.0500 ± 0.7057 ms\n","CPU usage: 43.7610% ± 620.2183%\n","RAM change over 1000 runs: 3664.0000 KB\n","RAM change per sample: 3.6640 KB\n","PCA-CNN model size: 0.0194 MB\n","```\n","\n","\n","```\n","Results for Baseline Float32 Model over 1000 runs:\n","Inference wall-clock time: 0.0746 ± 0.0283 ms\n","CPU time: 0.0900 ± 0.9449 ms\n","CPU usage: 58.4758% ± 641.9946%\n","RAM change over 1000 runs: 3556.0000 KB\n","RAM change per sample: 3.5560 KB\n","PCA-CNN model size: 0.0194 MB\n","```\n","\n","\n","\n","\n","\n"],"metadata":{"id":"0sXDAkHHMwDw"}},{"cell_type":"markdown","source":["#FP16 Baseline CNN\n","\n"],"metadata":{"id":"esGX2qiRH59C"}},{"cell_type":"code","source":["import os\n","import time\n","import numpy as np\n","import psutil\n","import tensorflow as tf\n","import gc\n","import statistics\n","\n","# -------------------\n","# --- Load models ---\n","# -------------------\n","\n","baseline_model_base = os.path.join(save_path, \"Best_Baseline\")\n","\n","tflite_models = [\n","    {\n","        \"model_path\": baseline_model_base + \"_fp16_weights.tflite\",\n","        \"model_name\": \"Baseline Float16 Model\",\n","    }\n","]\n","\n","process = psutil.Process(os.getpid()) # Initialise process instance\n","\n","# ----------------------------\n","# --- Run multiple trials ---\n","# ----------------------------\n","\n","runs = 1000\n","for m in tflite_models:\n","    start_mem = process.memory_info().rss / (1024**2)\n","\n","    # --- Load interpreters ---\n","    interpreter = tf.lite.Interpreter(model_path=m[\"model_path\"])\n","    interpreter.allocate_tensors()\n","    input_details = interpreter.get_input_details()\n","    output_details = interpreter.get_output_details()\n","\n","    wall_times, the_cpu_seconds = [], []\n","\n","    for _ in range(runs):\n","\n","        # Record resource usage\n","        start_time = time.time()\n","        start_cpu_times = process.cpu_times()\n","\n","        # --- Inference ---\n","        sample_input = np.expand_dims(X_test_sample, axis=0)   # (1, features)\n","        sample_input = np.expand_dims(sample_input, axis=-1)   # (1, features, 1)\n","        sample_input = sample_input.astype(input_details[0]['dtype'])\n","\n","        interpreter.set_tensor(input_details[0]['index'], sample_input)\n","        interpreter.invoke()\n","\n","        output_data = interpreter.get_tensor(output_details[0]['index'])\n","        y_pred = np.argmax(output_data, axis=1)\n","\n","        # Record resource usage\n","        end_time = time.time()\n","        end_cpu_times = process.cpu_times()\n","        end_mem = process.memory_info().rss / (1024**2)\n","\n","        # Compute metric totals for a single sample\n","        wall_clock = end_time - start_time\n","        cpu_seconds = (end_cpu_times.user + end_cpu_times.system) - (start_cpu_times.user + start_cpu_times.system)\n","\n","        #Add samples' runs times to the lists\n","        wall_times.append(wall_clock)\n","        the_cpu_seconds.append(cpu_seconds)\n","\n","    # Memory usage\n","    end_mem = process.memory_info().rss / (1024**2)\n","    total_ram_change = end_mem - start_mem\n","    ram_change_per_sample = total_ram_change / runs\n","\n","    # --- Report average results ---\n","    print(f\"\\nResults for {m['model_name']} over {runs} runs:\")\n","    print(f\"Inference wall-clock time: {statistics.mean(wall_times):.10f} ± {statistics.stdev(wall_times):.10f} sec\")\n","    print(f\"CPU seconds: {statistics.mean(the_cpu_seconds):.10f} ± {statistics.stdev(the_cpu_seconds):.10f} sec\")\n","\n","    print(f\"RAM change: {total_ram_change:.10f} MB\")\n","    print(f\"RAM change per sample: {ram_change_per_sample:.10f} MB\")\n","\n","    print(f\"\\nResults for {m['model_name']} over {runs} runs:\")\n","    print(f\"Inference wall-clock time: {statistics.mean(wall_times)*1000:.4f} ± {statistics.stdev(wall_times)*1000:.4f} ms\")\n","    print(f\"CPU time: {statistics.mean(the_cpu_seconds)*1000:.4f} ± {statistics.stdev(the_cpu_seconds)*1000:.4f} ms\")\n","\n","    print(f\"RAM change over {runs} runs: {total_ram_change*1024:.4f} KB\")\n","    print(f\"RAM change per sample: {ram_change_per_sample*1024:.4f} KB\")\n","\n","    # Model file sizes\n","    baseline_size_mb = os.path.getsize(m[\"model_path\"]) / (1024**2)\n","    print(f\"Baseline fp16 model size: {baseline_size_mb:.4f} MB\")\n","\n"],"metadata":{"id":"l6jhI_ix03FN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["FP16 Baseline CNN Results\n","\n","\n","\n","```\n","Results for Baseline Float16 Model over 1000 runs:\n","Inference wall-clock time: 0.0763 ± 0.0297 ms\n","CPU time: 0.0900 ± 0.9449 ms\n","CPU usage: 73.2113% ± 784.1949%\n","RAM change over 1000 runs: 3620.0000 KB\n","RAM change per sample: 3.6200 KB\n","PCA-CNN model size: 0.0145 MB\n","```\n","\n","\n","```\n","Results for Baseline Float16 Model over 1000 runs:\n","Inference wall-clock time: 0.0652 ± 0.0231 ms\n","CPU time: 0.1000 ± 0.9955 ms\n","CPU usage: 81.0780% ± 814.2452%\n","RAM change over 1000 runs: 3584.0000 KB\n","RAM change per sample: 3.5840 KB\n","PCA-CNN model size: 0.0145 MB\n","```\n","\n","\n","```\n","Results for Baseline Float16 Model over 1000 runs:\n","Inference wall-clock time: 0.0881 ± 0.0282 ms\n","CPU time: 0.0700 ± 0.8341 ms\n","CPU usage: 35.2668% ± 442.9308%\n","RAM change over 1000 runs: 3460.0000 KB\n","RAM change per sample: 3.4600 KB\n","PCA-CNN model size: 0.0145 MB\n","```\n","\n","\n","\n","```\n","Results for Baseline Float16 Model over 1000 runs:\n","Inference wall-clock time: 0.0632 ± 0.0208 ms\n","CPU time: 0.0600 ± 0.7727 ms\n","CPU usage: 54.2956% ± 699.3228%\n","RAM change over 1000 runs: 3636.0000 KB\n","RAM change per sample: 3.6360 KB\n","PCA-CNN model size: 0.0145 MB\n","```\n","\n","\n","\n","```\n","Results for Baseline Float16 Model over 1000 runs:\n","Inference wall-clock time: 0.0964 ± 0.0366 ms\n","CPU time: 0.1000 ± 0.9955 ms\n","CPU usage: 50.9837% ± 510.2325%\n","RAM change over 1000 runs: 3680.0000 KB\n","RAM change per sample: 3.6800 KB\n","PCA-CNN model size: 0.0145 MB\n","```\n","\n","\n","\n","\n","\n"],"metadata":{"id":"3CKOP8EoReiU"}},{"cell_type":"markdown","source":["#Dynamic INT8 Baseline CNN\n","\n","\n"],"metadata":{"id":"6gtH0zoLM0jb"}},{"cell_type":"code","source":["import os\n","import time\n","import numpy as np\n","import psutil\n","import tensorflow as tf\n","import gc\n","import statistics\n","\n","# -------------------\n","# --- Load models ---\n","# -------------------\n","\n","baseline_model_base = os.path.join(save_path, \"Best_Baseline\")\n","\n","tflite_models = [\n","    {\n","        \"model_path\": baseline_model_base + \"_int8_weights.tflite\",\n","        \"model_name\": \"Baseline CNN Dyamic INT8 Model\",\n","    }\n","]\n","\n","process = psutil.Process(os.getpid()) #Initialise process instance\n","# ----------------------------\n","# --- Run multiple trials ---\n","# ----------------------------\n","runs = 1000\n","for m in tflite_models:\n","    start_mem = process.memory_info().rss / (1024**2)\n","\n","    # --- Load interpreters ---\n","    interpreter = tf.lite.Interpreter(model_path=m[\"model_path\"])\n","    interpreter.allocate_tensors()\n","    input_details = interpreter.get_input_details()\n","    output_details = interpreter.get_output_details()\n","\n","    wall_times, the_cpu_seconds = [], []\n","\n","    for _ in range(runs):\n","        # Record resource usage start stats\n","        start_time = time.time()\n","        start_cpu_times = process.cpu_times()\n","\n","\n","        # --- Inference ---\n","        sample_input = np.expand_dims(X_test_sample, axis=0)   # (1, features)\n","        sample_input = np.expand_dims(sample_input, axis=-1)   # (1, features, 1)\n","        sample_input = sample_input.astype(input_details[0]['dtype'])\n","\n","        interpreter.set_tensor(input_details[0]['index'], sample_input)\n","        interpreter.invoke()\n","        output_data = interpreter.get_tensor(output_details[0]['index'])\n","        y_pred = np.argmax(output_data, axis=1)\n","\n","        # Record resource usage\n","        end_time = time.time()\n","        end_cpu_times = process.cpu_times()\n","        end_mem = process.memory_info().rss / (1024**2)\n","\n","        # Compute resource usage totals\n","        wall_clock = end_time - start_time\n","        cpu_seconds = (end_cpu_times.user + end_cpu_times.system) - (start_cpu_times.user + start_cpu_times.system)\n","\n","        # Append to lists\n","        wall_times.append(wall_clock)\n","        the_cpu_seconds.append(cpu_seconds)\n","\n","    # Compute Memory usage\n","    end_mem = process.memory_info().rss / (1024**2)\n","    total_ram_change = end_mem - start_mem\n","    ram_change_per_sample = total_ram_change / runs\n","\n","    # --- Report average results ---\n","    print(f\"\\nResults for {m['model_name']} over {runs} runs:\")\n","    print(f\"Inference wall-clock time: {statistics.mean(wall_times):.10f} ± {statistics.stdev(wall_times):.10f} sec\")\n","    print(f\"CPU seconds: {statistics.mean(the_cpu_seconds):.10f} ± {statistics.stdev(the_cpu_seconds):.10f} sec\")\n","\n","    print(f\"RAM change: {total_ram_change:.10f} MB\")\n","    print(f\"RAM change per sample: {ram_change_per_sample:.10f} MB\")\n","\n","    print(f\"\\nResults for {m['model_name']} over {runs} runs:\")\n","    print(f\"Inference wall-clock time: {statistics.mean(wall_times)*1000:.4f} ± {statistics.stdev(wall_times)*1000:.4f} ms\")\n","    print(f\"CPU time: {statistics.mean(the_cpu_seconds)*1000:.4f} ± {statistics.stdev(the_cpu_seconds)*1000:.4f} ms\")\n","\n","    print(f\"RAM change over {runs} runs: {total_ram_change*1024:.4f} KB\")\n","    print(f\"RAM change per sample: {ram_change_per_sample*1024:.4f} KB\")\n","\n","    # Model file sizes\n","    baseline_size_mb = os.path.getsize(m[\"model_path\"]) / (1024**2)\n","    print(f\"Baseline Dynamic int8 model size: {baseline_size_mb:.4f} MB\")\n","\n"],"metadata":{"id":"rxIj37vjXH_c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Dynamic INT8 Results\n","\n","\n","\n","```\n","Results for Baseline CNN Dyamic INT8 Model over 1000 runs:\n","Inference wall-clock time: 0.0757 ± 0.0318 ms\n","CPU time: 0.1000 ± 0.9955 ms\n","CPU usage: 72.5048% ± 746.5935%\n","RAM change over 1000 runs: 3488.0000 KB\n","RAM change per sample: 3.4880 KB\n","baseline Dynamic int8 model size: 0.0131 MB\n","```\n","\n","\n","\n","```\n","Results for Baseline CNN Dyamic INT8 Model over 1000 runs:\n","Inference wall-clock time: 0.0649 ± 0.0216 ms\n","CPU time: 0.0700 ± 0.8341 ms\n","CPU usage: 61.7347% ± 735.7520%\n","RAM change over 1000 runs: 3548.0000 KB\n","RAM change per sample: 3.5480 KB\n","baseline Dynamic int8 model size: 0.0131 MB\n","```\n","\n","\n","```\n","Results for Baseline CNN Dyamic INT8 Model over 1000 runs:\n","Inference wall-clock time: 0.0928 ± 0.0403 ms\n","CPU time: 0.0900 ± 0.9449 ms\n","CPU usage: 41.4975% ± 444.1486%\n","RAM change over 1000 runs: 3492.0000 KB\n","RAM change per sample: 3.4920 KB\n","baseline Dynamic int8 model size: 0.0131 MB\n","```\n","\n","\n","\n","```\n","Results for Baseline CNN Dyamic INT8 Model over 1000 runs:\n","Inference wall-clock time: 0.0790 ± 0.0372 ms\n","CPU time: 0.1100 ± 1.0435 ms\n","CPU usage: 68.6196% ± 688.7047%\n","RAM change over 1000 runs: 3656.0000 KB\n","RAM change per sample: 3.6560 KB\n","baseline Dynamic int8 model size: 0.0131 MB\n","```\n","\n","\n","\n","```\n","Results for Baseline CNN Dyamic INT8 Model over 1000 runs:\n","Inference wall-clock time: 0.0684 ± 0.0260 ms\n","CPU time: 0.0800 ± 0.8913 ms\n","CPU usage: 61.0650% ± 691.0112%\n","RAM change over 1000 runs: 3020.0000 KB\n","RAM change per sample: 3.0200 KB\n","baseline Dynamic int8 model size: 0.0131 MB\n","```\n","\n","\n","\n"],"metadata":{"id":"qvka5IseXV37"}},{"cell_type":"markdown","source":["#Full INT8 Baseline CNN\n","\n","\n"],"metadata":{"id":"7h1gtoJUNCbb"}},{"cell_type":"code","source":["import os\n","import time\n","import numpy as np\n","import psutil\n","import tensorflow as tf\n","import gc\n","import statistics\n","\n","# -------------------\n","# --- Load models ---\n","# -------------------\n","baseline_model_base = os.path.join(save_path, \"Best_Baseline\")\n","\n","tflite_models = [\n","    {\n","        \"model_path\": baseline_model_base + \"_int8_full.tflite\",\n","        \"model_name\": \"Baseline CNN Full INT8 Model\",\n","    }\n","]\n","\n","process = psutil.Process(os.getpid()) #Initialise process instance\n","\n","# ----------------------------\n","# --- Run multiple trials ---\n","# ----------------------------\n","runs = 1000\n","for m in tflite_models:\n","    start_mem = process.memory_info().rss / (1024**2)\n","\n","    # --- Load interpreters ---\n","    interpreter = tf.lite.Interpreter(model_path=m[\"model_path\"])\n","    interpreter.allocate_tensors()\n","    input_details = interpreter.get_input_details()\n","    output_details = interpreter.get_output_details()\n","\n","    wall_times, the_cpu_seconds = [], []\n","\n","    for _ in range(runs):\n","\n","        # Record resource usage\n","        start_time = time.time()\n","        start_cpu_times = process.cpu_times()\n","\n","        # --- Inference ---\n","        # If a model is designed to process data with int8 activations, the input data must be quantised to INT8 before it is fed into the model\n","        # Get quantisation parameters (scale and zero point) to convert input data to int8 for model to process\n","        in_scale, in_zp = input_details[0]['quantization']\n","        out_scale, out_zp = output_details[0]['quantization']\n","\n","        # Shape Input\n","        sample_input = np.expand_dims(X_test_sample, axis=0)   # (1, features)\n","        sample_input = np.expand_dims(sample_input, axis=-1)   # (1, features, 1)\n","\n","        # Quantise input correctly for int8 model\n","        if input_details[0]['dtype'] == np.int8:\n","            quant_sample_input = np.round((sample_input / in_scale) + in_zp).astype(np.int8) # x_q = round(x/S + Z)\n","\n","        interpreter.set_tensor(input_details[0]['index'], quant_sample_input)\n","        interpreter.invoke()\n","        quant_sample_output = interpreter.get_tensor(output_details[0]['index'])\n","\n","        # Dequantise outputs back to float\n","        if output_details[0]['dtype'] == np.int8:\n","            output_data = (quant_sample_output.astype(np.float32) - out_zp) * out_scale # x = S * (x_q - Z)\n","\n","        y_pred = np.argmax(output_data, axis=1)\n","\n","        # Record resource usage\n","        end_time = time.time()\n","        end_cpu_times = process.cpu_times()\n","        end_mem = process.memory_info().rss / (1024**2)\n","\n","        # Compute resouce totals\n","        wall_clock = end_time - start_time\n","        cpu_seconds = (end_cpu_times.user + end_cpu_times.system) - (start_cpu_times.user + start_cpu_times.system)\n","\n","        # Add time measurements to lists\n","        wall_times.append(wall_clock)\n","        the_cpu_seconds.append(cpu_seconds)\n","\n","    # Get memory usage\n","    end_mem = process.memory_info().rss / (1024**2)\n","    total_ram_change = end_mem - start_mem\n","    ram_change_per_sample = total_ram_change / runs\n","\n","    # --- Report average results ---s\n","    print(f\"\\nResults for {m['model_name']} over {runs} runs:\")\n","    print(f\"Inference wall-clock time: {statistics.mean(wall_times):.10f} ± {statistics.stdev(wall_times):.10f} sec\")\n","    print(f\"CPU seconds: {statistics.mean(the_cpu_seconds):.10f} ± {statistics.stdev(the_cpu_seconds):.10f} sec\")\n","\n","    print(f\"RAM change: {total_ram_change:.10f} MB\")\n","    print(f\"RAM change per sample: {ram_change_per_sample:.10f} MB\")\n","\n","    print(f\"\\nResults for {m['model_name']} over {runs} runs:\")\n","    print(f\"Inference wall-clock time: {statistics.mean(wall_times)*1000:.4f} ± {statistics.stdev(wall_times)*1000:.4f} ms\")\n","    print(f\"CPU time: {statistics.mean(the_cpu_seconds)*1000:.4f} ± {statistics.stdev(the_cpu_seconds)*1000:.4f} ms\")\n","\n","    print(f\"RAM change over {runs} runs: {total_ram_change*1024:.4f} KB\")\n","    print(f\"RAM change per sample: {ram_change_per_sample*1024:.4f} KB\")\n","\n","    # Model file sizes\n","    baseline_size_mb = os.path.getsize(m[\"model_path\"]) / (1024**2)\n","    print(f\"Baseline Full int8 model size: {baseline_size_mb:.4f} MB\")\n"],"metadata":{"id":"QwhoyovnNEx8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Full INT8 Results\n","\n","\n","\n","```\n","Results for Baseline CNN Full INT8 Model over 1000 runs:\n","Inference wall-clock time: 0.0989 ± 0.0359 ms\n","CPU time: 0.1000 ± 0.9955 ms\n","CPU usage: 46.6113% ± 481.1900%\n","RAM change over 1000 runs: 1772.0000 KB\n","RAM change per sample: 1.7720 KB\n","Baseline Full int8 model size: 0.0141 MB\n","```\n","\n","```\n","Results for Baseline CNN Full INT8 Model over 1000 runs:\n","Inference wall-clock time: 0.1341 ± 0.0448 ms\n","CPU time: 0.1100 ± 1.0435 ms\n","CPU usage: 43.7959% ± 419.1954%\n","RAM change over 1000 runs: 2528.0000 KB\n","RAM change per sample: 2.5280 KB\n","Baseline Full int8 model size: 0.0141 MB\n","```\n","\n","```\n","Results for Baseline CNN Full INT8 Model over 1000 runs:\n","Inference wall-clock time: 0.1041 ± 0.0362 ms\n","CPU time: 0.0800 ± 0.8913 ms\n","CPU usage: 38.5336% ± 453.6401%\n","RAM change over 1000 runs: 2228.0000 KB\n","RAM change per sample: 2.2280 KB\n","Baseline Full int8 model size: 0.0141 MB\n","```\n","\n","\n","```\n","Results for Baseline CNN Full INT8 Model over 1000 runs:\n","Inference wall-clock time: 0.1110 ± 0.0956 ms\n","CPU time: 0.1200 ± 1.0894 ms\n","CPU usage: 66.5872% ± 617.5510%\n","RAM change over 1000 runs: 2184.0000 KB\n","RAM change per sample: 2.1840 KB\n","Baseline Full int8 model size: 0.0141 MB\n","```\n","\n","\n","```\n","Results for Baseline CNN Full INT8 Model over 1000 runs:\n","Inference wall-clock time: 0.8319 ± 2.9436 ms\n","CPU time: 0.1700 ± 1.2934 ms\n","CPU usage: 37.3612% ± 330.3989%\n","RAM change over 1000 runs: 2624.0000 KB\n","RAM change per sample: 2.6240 KB\n","Baseline Full int8 model size: 0.0141 MB\n","```\n","\n","\n","\n","\n","\n"],"metadata":{"id":"02KmCjNMXIbc"}}]}