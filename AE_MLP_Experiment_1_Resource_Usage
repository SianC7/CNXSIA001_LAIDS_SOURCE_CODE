{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Run Instructions:\n","\n","\n","\n","*   Connect to google drive to access files.\n","*   Change file paths to where the datasets (dataset_path) and model files (model_path) are stored.\n","*   Run one model at a time (comment other model code out) to get accurate measurements.\n","*   Restart runtime environement ('Restart session and run all') before every measurement is taken.\n","\n"],"metadata":{"id":"uqPvnsxIZbFr"}},{"cell_type":"markdown","source":["#Mount Google Drive"],"metadata":{"id":"2cZmMCkNEaex"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"m8WPMUHwSryH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1758119212894,"user_tz":-120,"elapsed":24352,"user":{"displayName":"Sian C","userId":"07859676698101222721"}},"outputId":"94607e76-f24c-423c-9509-b31a58c2a9aa"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["#File Paths"],"metadata":{"id":"WrUkHWOPA0Ij"}},{"cell_type":"code","source":["model_path = \"/content/drive/MyDrive/CNXSIA001_LAIDS_SOURCE_CODE/AE-MLP Model Files/Models\"\n","dataset_path =  \"/content/drive/MyDrive/CNXSIA001_LAIDS_SOURCE_CODE/AE-MLP Model Files/Datasets and Numpy Arrays\""],"metadata":{"id":"Eqpx_eRoA-6X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import os\n","import numpy as np\n","\n","# Test sets\n","X_test = pd.read_csv(os.path.join(dataset_path, \"ae_mlp_x_test.csv\")).to_numpy()\n","y_test = pd.read_csv(os.path.join(dataset_path, \"ae_mlp_y_test.csv\")).to_numpy().ravel()\n","\n","# Get index of first malicious sample\n","first_malicious_index = np.where(y_test != 0)[0][0]\n","print('First malicious sample\\'s index: ',first_malicious_index)\n","\n","# Extract the sample\n","X_test_sample = X_test[first_malicious_index]\n","y_test_sample = y_test[first_malicious_index]\n","\n","print(f\"X_test_sample: {X_test_sample}\")\n","print(f\"y_test_sample: {y_test_sample}\")\n","\n","# Free up memory\n","del X_test, y_test\n","import gc; gc.collect()"],"metadata":{"id":"3b1doEX5Zfuy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Set CPU Constraints"],"metadata":{"id":"Pqmc4TasVi9A"}},{"cell_type":"code","source":["import os\n","# limit libraries' CPU thread requests to up to 4 threads max to use for parallel computations\n","\n","os.environ['TF_NUM_INTRAOP_THREADS'] = '4' # Only allow TensorFlow to request a max 4 CPU threads per operation (if only 2 threads exist, it only 2 threads are used)\n","os.environ['TF_NUM_INTEROP_THREADS'] = '1'  # Forces TensorFlow to run only one operation at a time\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # reduce Tensorflows output to the console\n","\n","os.environ['OMP_NUM_THREADS'] = '4' # Set the max number of threads the OpenMP (which uses numerical libraries such as NumPy) can request to 4\n","os.environ['MKL_NUM_THREADS'] = '4' # Set the maximum number of threads Intel MKL (Math Kernel Library) can request to 4\n","os.environ['OPENBLAS_NUM_THREADS'] = '4' # Set the maximum number of threads OpenBLAS can request to 4.\n","\n"],"metadata":{"id":"hp0k39XOl1TI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Imports"],"metadata":{"id":"D2NihRdLm-oV"}},{"cell_type":"code","source":["# ---------------\n","# --- Imports ---\n","# ---------------\n","\n","import os\n","import time\n","import resource\n","import psutil\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n","\n","import tensorflow as tf\n","from tensorflow.keras import layers, regularizers\n","from tensorflow.keras.models import load_model, Sequential\n","from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n"],"metadata":{"id":"Hprzx6shm_js"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Set resource constraints"],"metadata":{"id":"7RvkTflk3B2S"}},{"cell_type":"code","source":["# -------------------------------------------\n","# --- Google Colab Resource Configuration ---\n","# -------------------------------------------\n","\n","def configure_resources_for_colab():\n","    print(\"Configuring Google Colab resources...\")\n","\n","    # Restrict the number of CPU cores to use at most 4\n","    try:\n","        available_cores = min(4, os.cpu_count()) # Limit the CPU cores to be at most 4\n","        os.sched_setaffinity(0, list(range(available_cores))) #limit this current process (0) to run only on the list of available_cores CPU cores.\n","        print(\"Logical cores:\", psutil.cpu_count(logical=True))\n","        print(\"Physical cores:\", psutil.cpu_count(logical=False))\n","    except Exception as e:\n","        print(f\"CPU affinity warning: {e}\")\n","\n","    # Limit Memory to 4GB\n","    try:\n","        memory_limit = 4 * (1024**3)  # 4GB in bytes\n","        resource.setrlimit(resource.RLIMIT_AS, (memory_limit, memory_limit)) #sets the maximum allowed virtual memory this process can use to 4GB (set both soft and hard limits to 4 GB)\n","        print(f\"Memory limit set to 4GB\")\n","    except Exception as e:\n","        print(f\"Memory limit warning: {e}\")\n","\n","    # Print CPU informational\n","    try:\n","        cpu_info = psutil.cpu_freq()\n","        if cpu_info:\n","            print(f\"CPU frequency: {cpu_info.current:.0f}MHz\")\n","        else:\n","            print(f\"CPU frequency info not available\")\n","    except:\n","        print(f\"CPU frequency info not available\")\n","\n","    # Adjust CPU process priority\n","    try:\n","        os.nice(5)  # Lower the CPU priority of the process\n","        print(f\"Process priority lowered\")\n","    except Exception as e:\n","        print(f\"Priority adjustment warning: {e}\")\n","\n","    print(\"Resource configuration complete!\")\n","\n","# Execute function\n","configure_resources_for_colab()\n","\n","\n","# --------------------------------\n","# --- TensorFlow configuration ---\n","# --------------------------------\n","\n","def configure_tensorflow_colab():\n","    print(\"Configuring TensorFlow resources usage...\")\n","\n","    # Reenforce that tensorflow CPU thread usage is limited\n","    try:\n","        tf.config.threading.set_intra_op_parallelism_threads(4) # Only allow TensorFlow to request a max 4 CPU threads per operation (if only 2 threads exist, it only 2 threads are used)\n","        tf.config.threading.set_inter_op_parallelism_threads(1) # Forces TensorFlow to run only one operation at a time\n","        print(\"TensorFlow CPU thread limits configured\")\n","    except RuntimeError as e:\n","        print(\"CPU thread request error\")\n","\n","    tf.get_logger().setLevel('WARNING')# Limit the amount of info tf prints to the console\n","\n","    print(\"TensorFlow configuration complete!\")\n","\n","# Execute function\n","configure_tensorflow_colab()\n","\n","\n","# Get current Colab resource usages\n","def monitor_colab_resources():\n","    try:\n","        # Get memory usage\n","        mem = psutil.virtual_memory()\n","        print(f\"Memory: {mem.used / (1024**3):.1f}GB used out of {mem.total / (1024**3):.1f}GB total ({mem.percent:.1f}%)\")\n","\n","        # Get CPU usage\n","        cpu_percent = psutil.cpu_percent(interval=1)\n","        print(f\"CPU usage: {cpu_percent:.1f}%\")\n","\n","        # Get Disk space\n","        disk = psutil.disk_usage('/')\n","        print(f\"Disk: {disk.used / (1024**3):.1f}GB used out of {disk.total/(1024**3):.1f}GB total ({disk.percent:.1f}%)\")\n","\n","    except Exception as e:\n","        print(f\"Error: {e}\")\n","\n","# Execute function\n","monitor_colab_resources()"],"metadata":{"id":"uirBBsWgZB3U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Resource usage evaluation"],"metadata":{"id":"p8KozpUc213I"}},{"cell_type":"markdown","source":["#FP32 AE-MLP\n","\n","\n","\n"],"metadata":{"id":"RC-NBwn1H2w4"}},{"cell_type":"code","source":["import os\n","import time\n","import numpy as np\n","import psutil\n","import tensorflow as tf\n","import gc\n","import statistics\n","\n","# ----------------------------------------------\n","# --- Load pre-feature thresholds and models ---\n","# ----------------------------------------------\n","per_feature_thresholds_path = os.path.join(dataset_path, 'ae_per_feature_thresholds.npy')\n","per_feature_thresholds = np.load(per_feature_thresholds_path)\n","\n","ae_model_base = os.path.join(model_path, \"Best_AE\")\n","mlp_model_base = os.path.join(model_path, \"Best_MLP\")\n","\n","tflite_models = [\n","    {\n","        \"ae_path\": ae_model_base + \"_float32.tflite\",\n","        \"ae_name\": \"AE Float32 Model\",\n","        \"mlp_path\": mlp_model_base + \"_float32.tflite\",\n","        \"mlp_name\": \"MLP Float32 Model\",\n","    }\n","]\n","\n","process = psutil.Process(os.getpid()) #Initialise process instance\n","# ----------------------------\n","# --- Run multiple trials ---\n","# ----------------------------\n","runs = 1000\n","for m in tflite_models:\n","    start_mem = process.memory_info().rss / (1024**2) # Initial memeory meaurement\n","\n","    # --- Load interpreters ---\n","    ae_interpreter = tf.lite.Interpreter(model_path=m[\"ae_path\"])\n","    ae_interpreter.allocate_tensors()\n","    ae_input_details = ae_interpreter.get_input_details()\n","    ae_output_details = ae_interpreter.get_output_details()\n","\n","    mlp_interpreter = tf.lite.Interpreter(model_path=m[\"mlp_path\"])\n","    mlp_interpreter.allocate_tensors()\n","    mlp_input_details = mlp_interpreter.get_input_details()\n","    mlp_output_details = mlp_interpreter.get_output_details()\n","\n","    wall_times,the_cpu_seconds = [], []\n","\n","    for _ in range(runs):\n","        # Record resource usage\n","        start_time = time.time()\n","        start_cpu_times = process.cpu_times()\n","\n","        # --- AE inference ---\n","        input_data = np.expand_dims(X_test_sample, axis=0).astype(ae_input_details[0]['dtype'])\n","        ae_interpreter.set_tensor(ae_input_details[0]['index'], input_data)\n","        ae_interpreter.invoke()\n","        ae_output_data = ae_interpreter.get_tensor(ae_output_details[0]['index'])\n","\n","        test_reconstruction_errors = np.abs(ae_output_data[0] - X_test_sample)\n","        ae_y_pred = int((test_reconstruction_errors > per_feature_thresholds).any())\n","        malicious_pred_indices = np.flatnonzero(ae_y_pred)\n","\n","        # --- MLP inference ---\n","        if len(malicious_pred_indices) > 0:\n","            mlp_input = np.expand_dims(X_test_sample, axis=0).astype(mlp_input_details[0]['dtype'])\n","            mlp_interpreter.set_tensor(mlp_input_details[0]['index'], mlp_input)\n","            mlp_interpreter.invoke()\n","            mlp_output_data = mlp_interpreter.get_tensor(mlp_output_details[0]['index'])\n","\n","        # Record resource usage\n","        end_time = time.time()\n","        end_cpu_times = process.cpu_times()\n","\n","        # Compute precoessing time total\n","        wall_clock = end_time - start_time\n","        cpu_seconds = (end_cpu_times.user + end_cpu_times.system) - (start_cpu_times.user + start_cpu_times.system)\n","\n","        # Add time totals to ongoing list\n","        wall_times.append(wall_clock)\n","        the_cpu_seconds.append(cpu_seconds)\n","\n","\n","    end_mem = process.memory_info().rss / (1024**2) # Record end memory usage\n","    total_ram_change = end_mem - start_mem\n","    ram_change_per_sample = total_ram_change / runs\n","\n","    # --- Report average results ---\n","    print(f\"\\nResults for {m['ae_name']}-{m['mlp_name']} over {runs} runs:\")\n","    print(f\"Inference wall-clock time: {statistics.mean(wall_times):.10f} ± {statistics.stdev(wall_times):.10f} sec\")\n","    print(f\"CPU seconds: {statistics.mean(the_cpu_seconds):.10f} ± {statistics.stdev(the_cpu_seconds):.10f} sec\")\n","\n","    print(f\"RAM change: {total_ram_change:.10f} MB\")\n","    print(f\"RAM change per sample: {ram_change_per_sample:.10f} MB\")\n","\n","    # --- Report average results converted ---\n","    print(f\"\\nResults for {m['ae_name']}-{m['mlp_name']} over {runs} runs:\")\n","    print(f\"Inference wall-clock time: {statistics.mean(wall_times)*1000:.4f} ± {statistics.stdev(wall_times)*1000:.4f} ms\")\n","    print(f\"CPU time: {statistics.mean(the_cpu_seconds)*1000:.4f} ± {statistics.stdev(the_cpu_seconds)*1000:.4f} ms\")\n","\n","    print(f\"RAM change over {runs} runs: {total_ram_change*1024:.4f} KB\")\n","    print(f\"RAM change per sample: {ram_change_per_sample*1024:.4f} KB\")\n","\n","    # Model file sizes\n","    ae_size_mb = os.path.getsize(m[\"ae_path\"]) / (1024**2)\n","    mlp_size_mb = os.path.getsize(m[\"mlp_path\"]) / (1024**2)\n","    print(f\"AE model size: {ae_size_mb:.4f} MB\")\n","    print(f\"MLP model size: {mlp_size_mb:.4f} MB\")\n","    print(f\"Total storage size: {(ae_size_mb + mlp_size_mb):.4f} MB\")\n","\n"],"metadata":{"id":"xxRadtEMFl-p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["FP32 Results\n","\n","\n","```\n","Inference wall-clock time: 0.0001837423 ± 0.0005286753 sec\n","CPU seconds: 0.0001700000 ± 0.0012933572 sec\n","\n","Inference wall-clock time: 0.0000626409 ± 0.0000216413 sec\n","CPU seconds: 0.0000600000 ± 0.0007726558 sec\n","\n","Inference wall-clock time: 0.0000777769 ± 0.0000387422 sec\n","CPU seconds: 0.0000700000 ± 0.0008341438 sec\n","\n","Inference wall-clock time: 0.0000648744 ± 0.0000245191 sec\n","CPU seconds: 0.0000400000 ± 0.0006315052 sec\n","\n","Inference wall-clock time: 0.0001341784 ± 0.0002160180 sec\n","CPU seconds: 0.0000900000 ± 0.0009448771 sec\n","\n","```\n","\n","\n","\n","```\n","Results for AE Float32 Model + MLP Float32 Model over 1000 runs:\n","Inference wall-clock time: 0.0001 ± 0.0001 sec\n","CPU usage: 56.0231% ± 457.9271%\n","RAM change: 2.0000 MB\n","AE model size: 0.0518 MB\n","MLP model size: 0.0238 MB\n","Total storage size: 0.0756 MB\n","```\n","\n","```\n","Results for AE Float32 Model + MLP Float32 Model over 1000 runs:\n","Inference wall-clock time: 0.0001 ± 0.0000 sec\n","CPU usage: 41.3513% ± 533.7237%\n","RAM change: 3.0742 MB\n","AE model size: 0.0518 MB\n","MLP model size: 0.0238 MB\n","Total storage size: 0.0756 MB\n","```\n","\n","\n","```\n","Results for AE Float32 Model + MLP Float32 Model over 1000 runs:\n","Inference wall-clock time: 0.0001 ± 0.0000 sec\n","CPU usage: 83.5181% ± 792.4068%\n","RAM change: 3.2891 MB\n","AE model size: 0.0518 MB\n","MLP model size: 0.0238 MB\n","Total storage size: 0.0756 MB\n","```\n","\n","\n","```\n","Results for AE Float32 Model + MLP Float32 Model over 1000 runs:\n","Inference wall-clock time: 0.0001 ± 0.0000 sec\n","CPU usage: 70.2695% ± 684.5236%\n","RAM change: 3.2695 MB\n","AE model size: 0.0518 MB\n","MLP model size: 0.0238 MB\n","Total storage size: 0.0756 MB\n","```\n","\n","```\n","Results for AE Float32 Model + MLP Float32 Model over 1000 runs:\n","Inference wall-clock time: 0.0001 ± 0.0000 sec\n","CPU seconds: 0.0001% ± 0.0009%\n","CPU usage: 55.0748% ± 598.3193%\n","RAM change: 2.3984 MB\n","AE model size: 0.0518 MB\n","MLP model size: 0.0238 MB\n","Total storage size: 0.0756 MB\n","```\n","\n","\n","\n"],"metadata":{"id":"0sXDAkHHMwDw"}},{"cell_type":"markdown","source":["#FP16 AE-MLP\n","\n"],"metadata":{"id":"esGX2qiRH59C"}},{"cell_type":"code","source":["import os\n","import time\n","import numpy as np\n","import psutil\n","import tensorflow as tf\n","import gc\n","import statistics\n","\n","# ----------------------------------------------\n","# --- Load pre-feature thresholds and models ---\n","# ----------------------------------------------\n","per_feature_thresholds_path = os.path.join(dataset_path, 'ae_per_feature_thresholds.npy')\n","per_feature_thresholds = np.load(per_feature_thresholds_path)\n","\n","ae_model_base = os.path.join(model_path, \"Best_AE\")\n","mlp_model_base = os.path.join(model_path, \"Best_MLP\")\n","\n","tflite_models = [\n","    {\n","        \"ae_path\": ae_model_base + \"_fp16_weights.tflite\",\n","        \"ae_name\": \"AE Float16 Weights-Only Model\",\n","        \"mlp_path\": mlp_model_base + \"_fp16_weights.tflite\",\n","        \"mlp_name\": \"MLP Float16 Weights-Only Model\"\n","    }\n","]\n","\n","process = psutil.Process(os.getpid()) #Initialise process instance\n","# ----------------------------\n","# --- Run multiple trials ---\n","# ----------------------------\n","runs = 1000\n","for m in tflite_models:\n","    start_mem = process.memory_info().rss / (1024**2) # Initial memeory meaurement\n","\n","    # --- Load interpreters ---\n","    ae_interpreter = tf.lite.Interpreter(model_path=m[\"ae_path\"])\n","    ae_interpreter.allocate_tensors()\n","    ae_input_details = ae_interpreter.get_input_details()\n","    ae_output_details = ae_interpreter.get_output_details()\n","\n","    mlp_interpreter = tf.lite.Interpreter(model_path=m[\"mlp_path\"])\n","    mlp_interpreter.allocate_tensors()\n","    mlp_input_details = mlp_interpreter.get_input_details()\n","    mlp_output_details = mlp_interpreter.get_output_details()\n","\n","    wall_times,the_cpu_seconds = [], []\n","\n","    for _ in range(runs):\n","        # Record resource usage\n","        start_time = time.time()\n","        start_cpu_times = process.cpu_times()\n","\n","        # --- AE inference ---\n","        input_data = np.expand_dims(X_test_sample, axis=0).astype(ae_input_details[0]['dtype'])\n","        ae_interpreter.set_tensor(ae_input_details[0]['index'], input_data)\n","        ae_interpreter.invoke()\n","        ae_output_data = ae_interpreter.get_tensor(ae_output_details[0]['index'])\n","\n","        test_reconstruction_errors = np.abs(ae_output_data[0] - X_test_sample)\n","        ae_y_pred = int((test_reconstruction_errors > per_feature_thresholds).any())\n","        malicious_pred_indices = np.flatnonzero(ae_y_pred)\n","\n","        # --- MLP inference ---\n","        if len(malicious_pred_indices) > 0:\n","            mlp_input = np.expand_dims(X_test_sample, axis=0).astype(mlp_input_details[0]['dtype'])\n","            mlp_interpreter.set_tensor(mlp_input_details[0]['index'], mlp_input)\n","            mlp_interpreter.invoke()\n","            mlp_output_data = mlp_interpreter.get_tensor(mlp_output_details[0]['index'])\n","\n","        # Record resource usage\n","        end_time = time.time()\n","        end_cpu_times = process.cpu_times()\n","\n","        # Compute precoessing time total\n","        wall_clock = end_time - start_time\n","        cpu_seconds = (end_cpu_times.user + end_cpu_times.system) - (start_cpu_times.user + start_cpu_times.system)\n","\n","        # Add time totals to ongoing list\n","        wall_times.append(wall_clock)\n","        the_cpu_seconds.append(cpu_seconds)\n","\n","\n","    end_mem = process.memory_info().rss / (1024**2) # Record end memory usage\n","    total_ram_change = end_mem - start_mem\n","    ram_change_per_sample = total_ram_change / runs\n","\n","    # --- Report average results ---\n","    print(f\"\\nResults for {m['ae_name']}-{m['mlp_name']} over {runs} runs:\")\n","    print(f\"Inference wall-clock time: {statistics.mean(wall_times):.10f} ± {statistics.stdev(wall_times):.10f} sec\")\n","    print(f\"CPU seconds: {statistics.mean(the_cpu_seconds):.10f} ± {statistics.stdev(the_cpu_seconds):.10f} sec\")\n","\n","    print(f\"RAM change: {total_ram_change:.10f} MB\")\n","    print(f\"RAM change per sample: {ram_change_per_sample:.10f} MB\")\n","\n","    # --- Report average results converted ---\n","    print(f\"\\nResults for {m['ae_name']}-{m['mlp_name']} over {runs} runs:\")\n","    print(f\"Inference wall-clock time: {statistics.mean(wall_times)*1000:.4f} ± {statistics.stdev(wall_times)*1000:.4f} ms\")\n","    print(f\"CPU time: {statistics.mean(the_cpu_seconds)*1000:.4f} ± {statistics.stdev(the_cpu_seconds)*1000:.4f} ms\")\n","\n","    print(f\"RAM change over {runs} runs: {total_ram_change*1024:.4f} KB\")\n","    print(f\"RAM change per sample: {ram_change_per_sample*1024:.4f} KB\")\n","\n","    # Model file sizes\n","    ae_size_mb = os.path.getsize(m[\"ae_path\"]) / (1024**2)\n","    mlp_size_mb = os.path.getsize(m[\"mlp_path\"]) / (1024**2)\n","    print(f\"AE model size: {ae_size_mb:.4f} MB\")\n","    print(f\"MLP model size: {mlp_size_mb:.4f} MB\")\n","    print(f\"Total storage size: {(ae_size_mb + mlp_size_mb):.4f} MB\")\n","\n"],"metadata":{"id":"_eQe5_R-RUWx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["FP16 AE-MLP Results\n","\n","\n","\n","```\n","Inference wall-clock time: 0.0001353703 ± 0.0002156556 sec\n","CPU seconds: 0.0001200000 ± 0.0010893974 sec\n","\n","Inference wall-clock time: 0.0000700231 ± 0.0000364460 sec\n","CPU seconds: 0.0001200000 ± 0.0010893974 sec\n","\n","Inference wall-clock time: 0.0000690966 ± 0.0000346875 sec\n","CPU seconds: 0.0000600000 ± 0.0007726558 sec\n","\n","Inference wall-clock time: 0.0000690811 ± 0.0000327382 sec\n","CPU seconds: 0.0000400000 ± 0.0006315052 sec\n","\n","Inference wall-clock time: 0.0000661519 ± 0.0000327407 sec\n","CPU seconds: 0.0000800000 ± 0.0008912881 sec\n","```\n","\n","\n","\n","```\n","Results for AE Float16 Weights-Only Model + MLP Float16 Weights-Only Model over 1000 runs:\n","Inference wall-clock time: 0.0001 ± 0.0000 sec\n","CPU seconds: 0.0001% ± 0.0010%\n","CPU usage: 73.3242% ± 731.0077%\n","RAM change: 2.4453 MB\n","AE model size: 0.0304 MB\n","MLP model size: 0.0136 MB\n","Total storage size: 0.0440 MB\n","```\n","\n","\n","```\n","\n","Results for AE Float16 Weights-Only Model + MLP Float16 Weights-Only Model over 1000 runs:\n","Inference wall-clock time: 0.0001 ± 0.0000 sec\n","CPU seconds: 0.0001% ± 0.0009%\n","CPU usage: 56.8217% ± 606.8137%\n","RAM change: 2.2461 MB\n","AE model size: 0.0304 MB\n","MLP model size: 0.0136 MB\n","Total storage size: 0.0440 MB\n","```\n","\n","\n","```\n","Results for AE Float16 Weights-Only Model + MLP Float16 Weights-Only Model over 1000 runs:\n","Inference wall-clock time: 0.0001 ± 0.0001 sec\n","CPU seconds: 0.0001% ± 0.0012%\n","CPU usage: 68.0729% ± 564.6419%\n","RAM change: 2.5078 MB\n","AE model size: 0.0304 MB\n","MLP model size: 0.0136 MB\n","Total storage size: 0.0440 MB\n","```\n","\n","\n","```\n","Results for AE Float16 Weights-Only Model + MLP Float16 Weights-Only Model over 1000 runs:\n","Inference wall-clock time: 0.0001 ± 0.0001 sec\n","CPU seconds: 0.0002% ± 0.0012%\n","CPU usage: 53.3755% ± 460.4359%\n","RAM change: 3.1445 MB\n","AE model size: 0.0304 MB\n","MLP model size: 0.0136 MB\n","Total storage size: 0.0440 MB\n","```\n","\n","\n","\n","```\n","Results for AE Float16 Weights-Only Model + MLP Float16 Weights-Only Model over 1000 runs:\n","Inference wall-clock time: 0.0001 ± 0.0000 sec\n","CPU seconds: 0.0001% ± 0.0009%\n","CPU usage: 62.0322% ± 658.4898%\n","RAM change: 2.7617 MB\n","AE model size: 0.0304 MB\n","MLP model size: 0.0136 MB\n","Total storage size: 0.0440 MB\n","```\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"3CKOP8EoReiU"}},{"cell_type":"markdown","source":["#Dynamic INT8 AE-MLP\n","\n","\n"],"metadata":{"id":"6gtH0zoLM0jb"}},{"cell_type":"code","source":["import os\n","import time\n","import numpy as np\n","import psutil\n","import tensorflow as tf\n","import gc\n","import statistics\n","\n","# ----------------------------------------------\n","# --- Load pre-feature thresholds and models ---\n","# ----------------------------------------------\n","per_feature_thresholds_path = os.path.join(dataset_path, 'ae_per_feature_thresholds.npy')\n","per_feature_thresholds = np.load(per_feature_thresholds_path)\n","\n","ae_model_base = os.path.join(model_path, \"Best_AE\")\n","mlp_model_base = os.path.join(model_path, \"Best_MLP\")\n","\n","tflite_models = [\n","    {\n","        \"ae_path\": ae_model_base + \"_int8_weights.tflite\",\n","        \"ae_name\": \"AE Int8 Weights-Only Model\",\n","        \"mlp_path\": mlp_model_base + \"_int8_weights.tflite\",\n","        \"mlp_name\": \"MLP Int8 Weights-Only Model\"\n","    }\n","]\n","\n","process = psutil.Process(os.getpid()) #Initialise process instance\n","# ----------------------------\n","# --- Run multiple trials ---\n","# ----------------------------\n","runs = 1000\n","for m in tflite_models:\n","    start_mem = process.memory_info().rss / (1024**2) # Initial memeory meaurement\n","\n","    # --- Load interpreters ---\n","    ae_interpreter = tf.lite.Interpreter(model_path=m[\"ae_path\"])\n","    ae_interpreter.allocate_tensors()\n","    ae_input_details = ae_interpreter.get_input_details()\n","    ae_output_details = ae_interpreter.get_output_details()\n","\n","    mlp_interpreter = tf.lite.Interpreter(model_path=m[\"mlp_path\"])\n","    mlp_interpreter.allocate_tensors()\n","    mlp_input_details = mlp_interpreter.get_input_details()\n","    mlp_output_details = mlp_interpreter.get_output_details()\n","\n","    wall_times,the_cpu_seconds = [], []\n","\n","    for _ in range(runs):\n","        # Record resource usage\n","        start_time = time.time()\n","        start_cpu_times = process.cpu_times()\n","\n","        # --- AE inference ---\n","        input_data = np.expand_dims(X_test_sample, axis=0).astype(ae_input_details[0]['dtype'])\n","        ae_interpreter.set_tensor(ae_input_details[0]['index'], input_data)\n","        ae_interpreter.invoke()\n","        ae_output_data = ae_interpreter.get_tensor(ae_output_details[0]['index'])\n","\n","        test_reconstruction_errors = np.abs(ae_output_data[0] - X_test_sample)\n","        ae_y_pred = int((test_reconstruction_errors > per_feature_thresholds).any())\n","        malicious_pred_indices = np.flatnonzero(ae_y_pred)\n","\n","        # --- MLP inference ---\n","        if len(malicious_pred_indices) > 0:\n","            mlp_input = np.expand_dims(X_test_sample, axis=0).astype(mlp_input_details[0]['dtype'])\n","            mlp_interpreter.set_tensor(mlp_input_details[0]['index'], mlp_input)\n","            mlp_interpreter.invoke()\n","            mlp_output_data = mlp_interpreter.get_tensor(mlp_output_details[0]['index'])\n","\n","        # Record resource usage\n","        end_time = time.time()\n","        end_cpu_times = process.cpu_times()\n","\n","        # Compute precoessing time total\n","        wall_clock = end_time - start_time\n","        cpu_seconds = (end_cpu_times.user + end_cpu_times.system) - (start_cpu_times.user + start_cpu_times.system)\n","\n","        # Add time totals to ongoing list\n","        wall_times.append(wall_clock)\n","        the_cpu_seconds.append(cpu_seconds)\n","\n","\n","    end_mem = process.memory_info().rss / (1024**2) # Record end memory usage\n","    total_ram_change = end_mem - start_mem\n","    ram_change_per_sample = total_ram_change / runs\n","\n","    # --- Report average results ---\n","    print(f\"\\nResults for {m['ae_name']}-{m['mlp_name']} over {runs} runs:\")\n","    print(f\"Inference wall-clock time: {statistics.mean(wall_times):.10f} ± {statistics.stdev(wall_times):.10f} sec\")\n","    print(f\"CPU seconds: {statistics.mean(the_cpu_seconds):.10f} ± {statistics.stdev(the_cpu_seconds):.10f} sec\")\n","\n","    print(f\"RAM change: {total_ram_change:.10f} MB\")\n","    print(f\"RAM change per sample: {ram_change_per_sample:.10f} MB\")\n","\n","    # --- Report average results converted ---\n","    print(f\"\\nResults for {m['ae_name']}-{m['mlp_name']} over {runs} runs:\")\n","    print(f\"Inference wall-clock time: {statistics.mean(wall_times)*1000:.4f} ± {statistics.stdev(wall_times)*1000:.4f} ms\")\n","    print(f\"CPU time: {statistics.mean(the_cpu_seconds)*1000:.4f} ± {statistics.stdev(the_cpu_seconds)*1000:.4f} ms\")\n","\n","    print(f\"RAM change over {runs} runs: {total_ram_change*1024:.4f} KB\")\n","    print(f\"RAM change per sample: {ram_change_per_sample*1024:.4f} KB\")\n","\n","    # Model file sizes\n","    ae_size_mb = os.path.getsize(m[\"ae_path\"]) / (1024**2)\n","    mlp_size_mb = os.path.getsize(m[\"mlp_path\"]) / (1024**2)\n","    print(f\"AE model size: {ae_size_mb:.4f} MB\")\n","    print(f\"MLP model size: {mlp_size_mb:.4f} MB\")\n","    print(f\"Total storage size: {(ae_size_mb + mlp_size_mb):.4f} MB\")\n"],"metadata":{"id":"rxIj37vjXH_c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Dynamic INT8 Results\n","\n","\n","\n","```\n","Results for AE Int8 Weights-Only Model + MLP Int8 Weights-Only Model over 1000 runs:\n","Inference wall-clock time: 0.0001507260799407959 ± 0.00010533855225825758 sec\n","CPU seconds: 0.00015999999999999836% ± 0.0012553788177183587%\n","CPU usage: 52.4910% ± 412.9420%\n","RAM change: 2.1836 MB\n","AE model size: 0.0236 MB\n","MLP model size: 0.0096 MB\n","Total storage size: 0.0331 MB\n","```\n","\n","\n","\n","```\n","Results for AE Int8 Weights-Only Model + MLP Int8 Weights-Only Model over 1000 runs:\n","Inference wall-clock time: 0.0000759521 ± 0.0000208709 sec\n","CPU seconds: 0.0000700000% ± 0.0008341438%\n","CPU usage: 50.9263% ± 607.3189%\n","RAM change: 2.0469 MB\n","AE model size: 0.0236 MB\n","MLP model size: 0.0096 MB\n","Total storage size: 0.0331 MB\n","```\n","\n","\n","```\n","Results for AE Int8 Weights-Only Model + MLP Int8 Weights-Only Model over 1000 runs:\n","Inference wall-clock time: 0.0001206284 ± 0.0001551709 sec\n","CPU seconds: 0.0000900000 ± 0.0009448771 sec\n","CPU usage: 51.1102% ± 575.8026%\n","RAM change: 2.4375 MB\n","AE model size: 0.0236 MB\n","MLP model size: 0.0096 MB\n","Total storage size: 0.0331 MB\n","```\n","\n","```\n","Results for AE Int8 Weights-Only Model + MLP Int8 Weights-Only Model over 1000 runs:\n","Inference wall-clock time: 0.0000651555 ± 0.0000264898 sec\n","CPU seconds: 0.0000600000 ± 0.0007726558 sec\n","CPU usage: 41.1962% ± 546.5727%\n","RAM change: 2.4727 MB\n","AE model size: 0.0236 MB\n","MLP model size: 0.0096 MB\n","Total storage size: 0.0331 MB\n","```\n","\n","\n","\n","```\n","Results for AE Int8 Weights-Only Model + MLP Int8 Weights-Only Model over 1000 runs:\n","Inference wall-clock time: 0.0000670474 ± 0.0000297516 sec\n","CPU seconds: 0.0000800000 ± 0.0008912881 sec\n","CPU usage: 56.0917% ± 667.2226%\n","RAM change: 2.3789 MB\n","AE model size: 0.0236 MB\n","MLP model size: 0.0096 MB\n","Total storage size: 0.0331 MB\n","```\n","\n","\n","\n"],"metadata":{"id":"u1XRCp0dXSxm"}},{"cell_type":"markdown","source":["#Full INT8 AE-MLP\n","\n"],"metadata":{"id":"7h1gtoJUNCbb"}},{"cell_type":"code","source":["import os\n","import time\n","import numpy as np\n","import psutil\n","import tensorflow as tf\n","import gc\n","import statistics\n","# ----------------------------------\n","# --- Load thresholds and models ---\n","# ----------------------------------\n","\n","full_int8_thresholds_path = os.path.join(dataset_path, 'FULL_INT8_per_feature_Threshold.npy')\n","full_int8_thresholds = np.load(full_int8_thresholds_path)\n","\n","ae_model_base = os.path.join(model_path, \"Best_AE\")\n","mlp_model_base = os.path.join(model_path, \"Best_MLP\")\n","\n","tflite_models = [\n","    {\n","        \"ae_path\": ae_model_base + \"_int8_full.tflite\",\n","        \"ae_name\": \"AE Int8 Full Integer Model\",\n","        \"mlp_path\": mlp_model_base + \"_int8_full.tflite\",\n","        \"mlp_name\": \"MLP Int8 Full Integer Model\"\n","    }\n","]\n","\n","process = psutil.Process(os.getpid()) #Initialise process instance\n","\n","# ---------------------------------\n","# --- Evaluate Full INT8 AE-MLP ---\n","# ---------------------------------\n","\n","runs = 1000\n","for m in tflite_models:\n","    start_mem = process.memory_info().rss / (1024**2) # Initial memory measurement\n","\n","    # --- Load TFLite models ---\n","    ae_interpreter = tf.lite.Interpreter(model_path=m[\"ae_path\"])\n","    ae_interpreter.allocate_tensors()\n","    ae_input_details = ae_interpreter.get_input_details()\n","    ae_output_details = ae_interpreter.get_output_details()\n","\n","    mlp_interpreter = tf.lite.Interpreter(model_path=m[\"mlp_path\"])\n","    mlp_interpreter.allocate_tensors()\n","    mlp_input_details = mlp_interpreter.get_input_details()\n","    mlp_output_details = mlp_interpreter.get_output_details()\n","\n","    wall_times, the_cpu_seconds = [], [], []\n","\n","    for _ in range(runs):\n","      # Record start times\n","      start_time = time.time()\n","      start_cpu_times = process.cpu_times()\n","\n","      # --- Autoencoder Inference ---\n","      ae_input_scale, ae_input_zero_point = ae_input_details[0]['quantization']\n","      ae_output_scale, ae_output_zero_point = ae_output_details[0]['quantization']\n","\n","      input_sample_fp32 = np.expand_dims(X_test_sample, axis=0).astype(np.float32)\n","      input_sample_int8 = ((input_sample_fp32 / ae_input_scale) + ae_input_zero_point).astype(np.int8)\n","\n","      ae_interpreter.set_tensor(ae_input_details[0]['index'], input_sample_int8)\n","      ae_interpreter.invoke()\n","\n","      ae_output_int8 = ae_interpreter.get_tensor(ae_output_details[0]['index'])\n","      ae_output_fp32 = (ae_output_int8.astype(np.float32) - ae_output_zero_point) * ae_output_scale\n","\n","      test_reconstruction_errors = np.abs(ae_output_fp32[0] - X_test_sample)\n","      ae_y_pred = int((test_reconstruction_errors > full_int8_thresholds).any())\n","      malicious_pred_indices = np.flatnonzero(ae_y_pred)\n","\n","      # --- MLP Inference ---\n","      mlp_input_scale, mlp_input_zero_point = mlp_input_details[0]['quantization']\n","      mlp_output_scale, mlp_output_zero_point = mlp_output_details[0]['quantization']\n","\n","      mlp_y_pred_probs = []\n","      if len(malicious_pred_indices) > 0:\n","        mlp_input_fp32 = np.expand_dims(X_test_sample, axis=0).astype(np.float32)\n","        mlp_input_int8 = ((mlp_input_fp32 / mlp_input_scale) + mlp_input_zero_point).astype(np.int8)\n","\n","        mlp_interpreter.set_tensor(mlp_input_details[0]['index'], mlp_input_int8)\n","        mlp_interpreter.invoke()\n","\n","        mlp_output_int8 = mlp_interpreter.get_tensor(mlp_output_details[0]['index'])\n","        mlp_output_fp32 = (mlp_output_int8.astype(np.float32) - mlp_output_zero_point) * mlp_output_scale\n","\n","        mlp_y_pred_probs.append(mlp_output_fp32[0])\n","        mlp_y_pred = np.argmax(mlp_y_pred_probs, axis=1)\n","\n","        # Record end times\n","        end_time = time.time()\n","        end_cpu_times = process.cpu_times()\n","\n","        # Compute processing time totals\n","        wall_clock = end_time - start_time\n","        cpu_seconds = (end_cpu_times.user + end_cpu_times.system) - (start_cpu_times.user + start_cpu_times.system)\n","\n","        wall_times.append(wall_clock)\n","        the_cpu_seconds.append(cpu_seconds)\n","\n","    end_mem = process.memory_info().rss / (1024**2) # End memory measurement\n","    total_ram_change = end_mem - start_mem\n","    ram_change_per_sample = total_ram_change / runs\n","\n","    # --- Report average results ---\n","    print(f\"\\nResults for {m['ae_name']}-{m['mlp_name']} over {runs} runs:\")\n","    print(f\"Inference wall-clock time: {statistics.mean(wall_times):.10f} ± {statistics.stdev(wall_times):.10f} sec\")\n","    print(f\"CPU seconds: {statistics.mean(the_cpu_seconds):.10f} ± {statistics.stdev(the_cpu_seconds):.10f} sec\")\n","\n","    print(f\"RAM change: {total_ram_change:.10f} MB\")\n","    print(f\"RAM change per sample: {ram_change_per_sample:.10f} MB\")\n","\n","    # --- Report average results converted ---\n","    print(f\"\\nResults for {m['ae_name']}-{m['mlp_name']} over {runs} runs:\")\n","    print(f\"Inference wall-clock time: {statistics.mean(wall_times)*1000:.4f} ± {statistics.stdev(wall_times)*1000:.4f} ms\")\n","    print(f\"CPU time: {statistics.mean(the_cpu_seconds)*1000:.4f} ± {statistics.stdev(the_cpu_seconds)*1000:.4f} ms\")\n","\n","    print(f\"RAM change over {runs} runs: {total_ram_change*1024:.4f} KB\")\n","    print(f\"RAM change per sample: {ram_change_per_sample*1024:.4f} KB\")\n","\n","    # Model file sizes\n","    ae_size_mb = os.path.getsize(m[\"ae_path\"]) / (1024**2)\n","    mlp_size_mb = os.path.getsize(m[\"mlp_path\"]) / (1024**2)\n","    print(f\"AE model size: {ae_size_mb:.4f} MB\")\n","    print(f\"MLP model size: {mlp_size_mb:.4f} MB\")\n","    print(f\"Total storage size: {(ae_size_mb + mlp_size_mb):.4f} MB\")\n","\n"],"metadata":{"id":"QwhoyovnNEx8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Full INT8 Results\n","\n","\n","```\n","Results for AE Int8 Full Integer Model-MLP Int8 Full Integer Model over 1000 runs:\n","Inference wall-clock time: 0.1141 ± 0.1063 ms\n","CPU time: 0.1500 ± 1.2161 ms\n","CPU usage: 71.0323% ± 578.6783%\n","RAM change over 1000 runs: 2392.0000 KB\n","RAM change per sample: 2.3920 KB\n","AE model size: 0.0248 MB\n","MLP model size: 0.0104 MB\n","Total storage size: 0.0353 MB\n","```\n","\n","\n","\n","```\n","Results for AE Int8 Full Integer Model-MLP Int8 Full Integer Model over 1000 runs:\n","Inference wall-clock time: 0.1110 ± 0.0274 ms\n","CPU time: 0.1100 ± 1.0435 ms\n","CPU usage: 39.7288% ± 388.9358%\n","RAM change over 1000 runs: 2880.0000 KB\n","RAM change per sample: 2.8800 KB\n","AE model size: 0.0248 MB\n","MLP model size: 0.0104 MB\n","Total storage size: 0.0353 MB\n","```\n","\n","\n","```\n","Results for AE Int8 Full Integer Model-MLP Int8 Full Integer Model over 1000 runs:\n","Inference wall-clock time: 0.1143 ± 0.0455 ms\n","CPU time: 0.1100 ± 1.0435 ms\n","CPU usage: 43.7159% ± 423.0332%\n","RAM change over 1000 runs: 2716.0000 KB\n","RAM change per sample: 2.7160 KB\n","AE model size: 0.0248 MB\n","MLP model size: 0.0104 MB\n","Total storage size: 0.0353 MB\n","```\n","\n","\n","```\n","Results for AE Int8 Full Integer Model-MLP Int8 Full Integer Model over 1000 runs:\n","Inference wall-clock time: 0.1809 ± 0.0422 ms\n","CPU time: 0.1600 ± 1.2554 ms\n","CPU usage: 42.3665% ± 342.0349%\n","RAM change over 1000 runs: 2848.0000 KB\n","RAM change per sample: 2.8480 KB\n","AE model size: 0.0248 MB\n","MLP model size: 0.0104 MB\n","Total storage size: 0.0353 MB\n","```\n","\n","```\n","Results for AE Int8 Full Integer Model-MLP Int8 Full Integer Model over 1000 runs:\n","Inference wall-clock time: 0.1247 ± 0.0436 ms\n","CPU time: 0.1200 ± 1.0894 ms\n","CPU usage: 51.1128% ± 476.7526%\n","RAM change over 1000 runs: 2472.0000 KB\n","RAM change per sample: 2.4720 KB\n","AE model size: 0.0248 MB\n","MLP model size: 0.0104 MB\n","Total storage size: 0.0353 MB\n","```\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"lGS7D4oYVujC"}}]}